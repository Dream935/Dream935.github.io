<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>爬虫学习 | 小邹同学</title><meta name="author" content="Dream星辰"><meta name="copyright" content="Dream星辰"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="爬虫学习">
<meta property="og:type" content="article">
<meta property="og:title" content="爬虫学习">
<meta property="og:url" content="http://example.com/2023/10/14/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="小邹同学">
<meta property="og:description" content="爬虫学习">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img1.baidu.com/it/u=2628701950,2035046880&fm=253&fmt=auto&app=138&f=JPEG?w=500&h=288">
<meta property="article:published_time" content="2023-10-14T15:01:34.323Z">
<meta property="article:modified_time" content="2023-09-20T05:57:07.553Z">
<meta property="article:author" content="Dream星辰">
<meta property="article:tag" content="小邹同学个人博客">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img1.baidu.com/it/u=2628701950,2035046880&fm=253&fmt=auto&app=138&f=JPEG?w=500&h=288"><link rel="shortcut icon" href="/img/logo.png"><link rel="canonical" href="http://example.com/2023/10/14/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '爬虫学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-09-20 13:57:07'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/logo1.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://img1.baidu.com/it/u=2628701950,2035046880&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=500&amp;h=288')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">小邹同学</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">爬虫学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-10-14T15:01:34.323Z" title="发表于 2023-10-14 23:01:34">2023-10-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-09-20T05:57:07.553Z" title="更新于 2023-09-20 13:57:07">2023-09-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="爬虫学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h3 id="Scrapy-简介"><a href="#Scrapy-简介" class="headerlink" title="Scrapy 简介"></a>Scrapy 简介</h3><hr>
<p><code>Scrapy</code>是<code>Python</code>开发的一个快速、高层次的屏幕抓取和<code>web</code>抓取框架，用于抓取<code>web</code>站点并从页面中提取结构化的数据。<code>Scrapy</code>用途广泛，可以用于数据挖掘、监测和自动化测试。</p>
<p><code>Scrapy</code>吸引人的地方在于它是一个框架，任何人都可以根据需求方便的修改。它也提供了多种类型爬虫的基类，如<code>BaseSpider</code>、<code>sitemap</code>爬虫等，最新版本又提供了<code>web2.0</code>爬虫的支持。</p>
<h3 id="Scrapy-架构"><a href="#Scrapy-架构" class="headerlink" title="Scrapy 架构"></a>Scrapy 架构</h3><hr>
<p><img src="https://s1.ax1x.com/2020/03/20/8cOky8.png" alt="架构图"></p>
<p><strong>架构图</strong></p>
<ul>
<li><strong>Scrapy Engine(引擎)：</strong> 负责<code>Spider</code>、<code>ItemPipeline</code>、<code>Downloader</code>、<code>Scheduler</code>中间的通讯，信号、数据传递等。</li>
<li><strong>Scheduler(调度器)：</strong> 它负责接受引擎发送过来的<code>Request</code>请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。</li>
<li><strong>Downloader（下载器）：</strong> 负责下载 Scrapy <code>Engine</code>(引擎)发送的所有<code>Requests</code>请求，并将其获取到的<code>Responses</code>交还给<code>Scrapy Engine</code>(引擎)，由引擎交给<code>Spider</code>来处理。</li>
<li><strong>Spider（爬虫）：</strong> 它负责处理所有<code>Responses</code>,从中分析提取数据，获取<code>Item</code>字段需要的数据，并将需要跟进的 URL 提交给引擎，再次进入<code>Scheduler</code>(调度器)。</li>
<li><strong>Item Pipeline(管道)：</strong> 它负责处理<code>Spider</code>中获取到的<code>Item</code>，并进行进行后期处理（详细分析、过滤、存储等）的地方。</li>
<li><strong>Downloader Middlewares（下载中间件）：</strong> 一个可以自定义扩展下载功能的组件。</li>
<li><strong>Spider Middlewares（Spider 中间件）：</strong> 一个可以自定扩展和操作引擎和<code>Spider</code>中间通信的功能组件</li>
</ul>
<h3 id="Scrapy-原理"><a href="#Scrapy-原理" class="headerlink" title="Scrapy 原理"></a>Scrapy 原理</h3><hr>
<ol>
<li>引擎从调度器中取出一个链接（<code>URL</code>）用于接下来的抓取。</li>
<li>引擎把<code>URL</code>封装成一个请求（<code>Request</code>）传给下载器。</li>
<li>下载器把资源下载下来，并封装成应答包（<code>Response</code>）</li>
<li>爬虫解析<code>Response</code></li>
<li>解析出实体（<code>Item</code>），则交给实体管道进行进一步处理</li>
<li>解析出的是衔接（<code>URL</code>），则把<code>URL</code>交给调度器等待抓取</li>
</ol>
<h3 id="Scrapy-安装"><a href="#Scrapy-安装" class="headerlink" title="Scrapy 安装"></a>Scrapy 安装</h3><hr>
<p><strong>Windows 平台：</strong></p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1. pip3 install wheel</span><br><span class="line"></span><br><span class="line">2. 安装Twisted</span><br><span class="line">    进入http://www.lfd.uci.edu/~gohlke/pythonlibs/<span class="comment">#twisted</span></span><br><span class="line">    根据自身系统环境选择下载。</span><br><span class="line">    比如我的win10 64位 python3.7</span><br><span class="line">    则下载Twisted‑19.10.0‑cp37‑cp37m‑win_amd64.whl</span><br><span class="line">    进入本地下载后的文件夹</span><br><span class="line">    pip3 install Twisted‑19.10.0‑cp37‑cp37m‑win_amd64.whl</span><br><span class="line"></span><br><span class="line">3. pip3 install lxml</span><br><span class="line"></span><br><span class="line">4. pip3 install pyopenssl</span><br><span class="line"></span><br><span class="line">5. pip3 install pypiwin32</span><br><span class="line"></span><br><span class="line">6. pip3 install scrapy</span><br></pre></td></tr></table></figure>

<p><strong>Linux 平台：</strong></p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install Scrapy</span><br></pre></td></tr></table></figure>

<p><strong>pip 加速：</strong></p>
<blockquote>
<p>在国内<code>pip</code>下载速度特别慢，经常会下载失败，建议更改国内源下载。查看文章 –&gt; <a target="_blank" rel="noopener" href="https://www.cnblogs.com/schut/p/10410087.html">Python pip 配置国内源</a></p>
</blockquote>
<p><strong>命令行工具</strong><br>成功安装好<code>Scrapy</code>后，在命令行里输入<code>scrapy -h</code> 即可查看帮助</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1 查看帮助</span></span><br><span class="line">    scrapy -h</span><br><span class="line">    scrapy &lt;<span class="built_in">command</span>&gt; -h</span><br><span class="line"></span><br><span class="line"><span class="comment">#2 有两种命令：其中Project-only必须切到项目文件夹下才能执行，而Global的命令则不需要</span></span><br><span class="line">    Global commands:</span><br><span class="line">        startproject <span class="comment">#创建项目</span></span><br><span class="line">        genspider    <span class="comment">#创建爬虫程序</span></span><br><span class="line">        settings     <span class="comment">#如果是在项目目录下，则得到的是该项目的配置</span></span><br><span class="line">        runspider    <span class="comment">#运行一个独立的python文件，不必创建项目</span></span><br><span class="line">        shell        <span class="comment">#scrapy shell url地址  在交互式调试，如选择器规则正确与否</span></span><br><span class="line">        fetch        <span class="comment">#独立于程单纯地爬取一个页面，可以拿到请求头</span></span><br><span class="line">        view         <span class="comment">#下载完毕后直接弹出浏览器，以此可以分辨出哪些数据是ajax请求</span></span><br><span class="line">        version      <span class="comment">#scrapy version 查看scrapy的版本，scrapy version -v查看scrapy依赖库的版本</span></span><br><span class="line">    Project-only commands:</span><br><span class="line">        crawl        <span class="comment">#运行爬虫，必须创建项目才行，确保配置文件中ROBOTSTXT_OBEY = False</span></span><br><span class="line">        check        <span class="comment">#检测项目中有无语法错误</span></span><br><span class="line">        list         <span class="comment">#列出项目中所包含的爬虫名</span></span><br><span class="line">        edit         <span class="comment">#编辑器，一般不用</span></span><br><span class="line">        parse        <span class="comment">#scrapy parse url地址 --callback 回调函数  #以此可以验证我们的回调函数是否正确</span></span><br><span class="line">        bench        <span class="comment">#scrapy bentch压力测试</span></span><br></pre></td></tr></table></figure>

<h3 id="Scrapy-实战案例（单页面）"><a href="#Scrapy-实战案例（单页面）" class="headerlink" title="Scrapy 实战案例（单页面）"></a>Scrapy 实战案例（单页面）</h3><hr>
<ul>
<li>新建项目 ：新建一个新的爬虫项目</li>
<li>明确目标 ：明确你想要抓取的目标</li>
<li>制作爬虫 ：制作爬虫开始爬取网页</li>
<li>存储内容 ：设计管道存储爬取内容</li>
</ul>
<h4 id="步骤一-新建爬虫项目"><a href="#步骤一-新建爬虫项目" class="headerlink" title="步骤一 新建爬虫项目"></a>步骤一 新建爬虫项目</h4><p>新建一个名为 <code>mySpider</code> 的爬虫项目</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject mySpider</span><br></pre></td></tr></table></figure>

<p>输入完命令后，在当前目录下会出现一个<code>mySpider</code>的文件夹，目录结构如下</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mySpider/</span><br><span class="line">    scrapy.cfg</span><br><span class="line">    mySpider/</span><br><span class="line">        __init__.py</span><br><span class="line">        items.py</span><br><span class="line">        pipelines.py</span><br><span class="line">        settings.py</span><br><span class="line">        spiders/</span><br><span class="line">            __init__.py</span><br><span class="line">            ...</span><br></pre></td></tr></table></figure>

<p>文件说明：</p>
<ul>
<li><code>scrapy.cfg</code> 项目的配置信息。主要为 Scrapy 命令行工具提供一个基础的配置信息。（真正爬虫相关的配置信息在 settings.py 文件中）</li>
<li><code>mySpider/</code> 项目的 Python 模块，将会从这里引用代码。</li>
<li><code>mySpider/items.py</code> 项目的目标文件。设置数据存储模板，用于结构化数据。</li>
<li><code>mySpider/pipelines.py</code> 项目的管道文件。数据处理行为，如：一般结构化的数据持久化</li>
<li><code>mySpider/settings.py</code> 项目的配置文件，如：递归的层数、并发数，延迟下载等</li>
<li><code>mySpider/spiders/</code> 爬虫代码目录，如：创建文件，编写爬虫规则</li>
</ul>
<h4 id="步骤二-明确抓取目标"><a href="#步骤二-明确抓取目标" class="headerlink" title="步骤二 明确抓取目标"></a>步骤二 明确抓取目标</h4><ol>
<li>明确目标：爬取<code>http://www.itcast.cn/channel/teacher.shtml</code>页面中所有讲师的姓名、职称和个人信息。</li>
<li>打开编写 <code>mySpider</code> 目录下的 <code>items.py</code>。</li>
<li>自定义 姓名<code>name</code>、职称<code>title</code> 和 个人信息<code>info</code> 等字段</li>
</ol>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyspiderItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    info = scrapy.Field()</span><br></pre></td></tr></table></figure>

<h4 id="步骤三-制作爬虫代码"><a href="#步骤三-制作爬虫代码" class="headerlink" title="步骤三 制作爬虫代码"></a>步骤三 制作爬虫代码</h4><p>在 <code>mySpider/</code> 目录下输入命令，将会在 <code>mySpider/spider</code> 目录下自动生成一个名为 <code>itcast.py</code> 的爬虫文件，并指定爬取域的范围为 <code>itcast.cn</code> ，注意这里的爬虫名不能与项目名称起一样的。</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider itcast <span class="string">&quot;itcast.cn&quot;</span></span><br></pre></td></tr></table></figure>

<p>打开 <code>mySpider/spider</code> 目录里的 <code>itcast.py</code>，自动生成了下列代码:</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ItcastSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;itcast&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;itcast.cn&quot;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.itcast.cn/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>参数介绍：</p>
<ul>
<li><code>name = &quot;&quot;</code> ：这个爬虫的识别名称，必须是唯一的，在不同的爬虫必须定义不同的名字。</li>
<li><code>allow_domains = []</code> 是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页，不存在的 URL 会被忽略。</li>
<li><code>start_urls = ()</code> ：爬取的 URL 元祖&#x2F;列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些 urls 开始。其他子 URL 将会从这些起始 URL 中继承性生成。</li>
<li><code>parse(self, response)</code> ：负责解析返回的网页数据(response.body)，提取结构化数据(生成 item)，生成需要下一页的 URL 请求。</li>
</ul>
<p>导入刚才编写的<code>items.py</code>文件</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mySpider.items <span class="keyword">import</span> MyspiderItem</span><br></pre></td></tr></table></figure>

<p>将<code>start_urls</code>的值修改为需要爬取的初始<code>url</code></p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start_urls = [<span class="string">&quot;http://www.itcast.cn/channel/teacher.shtml&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>修改<code>parse()</code>方法，使用<code>XPath</code>语法对返回的<code>response</code>网页数据进行匹配提取</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    items = []</span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> response.xpath(<span class="string">&quot;//div[@class=&#x27;li_txt&#x27;]&quot;</span>): <span class="comment"># 循环匹配每个教师的数据</span></span><br><span class="line">        item = MyspiderItem() <span class="comment"># 实例化MyspiderItem类</span></span><br><span class="line">        name = each.xpath(<span class="string">&quot;h3/text()&quot;</span>).extract() <span class="comment"># 匹配姓名</span></span><br><span class="line">        title = each.xpath(<span class="string">&quot;h4/text()&quot;</span>).extract() <span class="comment"># 匹配职称</span></span><br><span class="line">        info = each.xpath(<span class="string">&quot;p/text()&quot;</span>).extract() <span class="comment"># 匹配个人信息</span></span><br><span class="line">        <span class="comment"># 将匹配到的值添加到item中</span></span><br><span class="line">        item[<span class="string">&#x27;name&#x27;</span>] = name[<span class="number">0</span>]</span><br><span class="line">        item[<span class="string">&#x27;title&#x27;</span>] = title[<span class="number">0</span>]</span><br><span class="line">        item[<span class="string">&#x27;info&#x27;</span>] = info[<span class="number">0</span>]</span><br><span class="line">        items.append(item) <span class="comment"># 将item中的值添加到items</span></span><br><span class="line">    <span class="keyword">return</span> items <span class="comment"># 函数返回items</span></span><br></pre></td></tr></table></figure>

<p><code>Scrapy</code>支持正则语法、<code>CSS</code>语法和<code>XPath</code>语法对网页内容进行匹配，这里推荐使用<code>XPath</code>语法，Scrapy 官网也是默认支持使用<code>XPath</code>语法。</p>
<p>以上<code>parse()</code>方法用到了<code>XPath</code>语法对指定内容进行匹配，如果有学过<code>CSS</code>，那么这里就非常容易理解<code>XPath</code>语法了。</p>
<blockquote>
<p>下面将简单对<code>XPath</code>语法举例介绍，更多详细点击查看<a target="_blank" rel="noopener" href="https://www.w3school.com.cn/xpath/index.asp">XPath 教程</a></p>
</blockquote>
<p>首先分析源代码，找到我们所需要的内容</p>
<p><img src="https://s1.ax1x.com/2020/03/20/82satJ.png" alt="源码分析"></p>
<p><strong>源码分析</strong></p>
<p>先介绍个简单的方法，可以直接在火狐或者谷歌浏览器中，找到相应的位置，鼠标右键-&gt;复制-&gt;<code>XPath</code>可以直接生成<code>XPath</code>语句。</p>
<p><img src="https://s1.ax1x.com/2020/03/20/82y454.png" alt="image"></p>
<p><strong>image</strong></p>
<p>复制生成的代码如下</p>
<p>html</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/html/body/div[1]/div[5]/div/div[2]/div[13]/ul/li[1]/div[2]</span><br></pre></td></tr></table></figure>

<p>这样方法生成的<code>XPath</code>语句非常的长，而且不容易理解。</p>
<p>接下来手动构造的<code>XPath</code>语句进行内容匹配</p>
<p>以上图源码为例</p>
<p>html</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;li_txt&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">h3</span>&gt;</span>于老师<span class="tag">&lt;/<span class="name">h3</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">h4</span>&gt;</span>高级讲师<span class="tag">&lt;/<span class="name">h4</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">p</span>&gt;</span></span><br><span class="line">    Java企业级应用专家、WEB技术专家，中科院软件工程硕士。07年起曾主持研发过多套软件培训课程与教材，精通JAVAEE、PHP、RUBY、JavaSCRIPT、RIA等多种主流开发语言，曾主持参与过中国联通UMMS二期工程等多个大项目。</span><br><span class="line">  <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>现在要匹配<code>class</code>为<code>li_txt</code>的<code>div</code>，构造<code>XPath</code>语句为</p>
<p>html</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//div[@class=&#x27;li_txt&#x27;]</span><br></pre></td></tr></table></figure>

<p>进一步匹配该<code>div</code>下的<code>h3</code>标签的内容，构造<code>XPath</code>语句为</p>
<p>html</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//div[@class=&#x27;li_txt&#x27;][1]/h3/text()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>从以上两个匹配例子可以很清楚的知道我们所要匹配的是什么，其中<code>//</code>表示任意的，后接<code>div</code>表示当前页面所有的<code>div</code>，而<code>div</code>后接的<code>[@class=&#39;li_txt&#39;]</code>表示指定<code>class</code>为<code>li_txt</code>的<code>div</code>,<code>/</code>表示该<code>div</code>紧接下一级内的子标签<code>h3</code>,<code>text()</code>表示当前<code>h3</code>中的文本内容</p>
</blockquote>
<p>到目前为止，已经成功使用了<code>XPath</code>语法，但是难免保证匹配到的都是正确的结果，所以接下来我们需要验证下我们的<code>XPath</code>是否正确</p>
<p>一种办法是使用谷歌浏览器的插件：<code>XPath Helper</code>。需要从谷歌网上应用店下载，如果没有梯子的话访问不了，这里推荐一个免费的梯子，有需要的可以下载<a target="_blank" rel="noopener" href="https://www.globetechnews.com/cn/?v=888#zero">佛跳墙</a>，连接上梯子网络后，即可访问<a target="_blank" rel="noopener" href="https://chrome.google.com/webstore?utm_source=chrome-ntp-icon">谷歌应用商店</a>搜索下载插件</p>
<p><img src="https://s1.ax1x.com/2020/03/20/8258wn.png" alt="XPath Helper"></p>
<p><strong>XPath Helper</strong></p>
<p>安装完插件后，点击右上角的<code>X</code>图标即可启动，然后在左边输入框中输入<code>XPath</code>语句，右边实时显示匹配的结果，清晰明了。</p>
<p>但此时<code>Scrapy</code>通过<code>response.xpath</code>(语法)不是直接获取字符串，我们需要将其转为字符串格式，通过以下方式可以得到字符串</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">response.xpath().get()  <span class="comment"># 得到一个元素</span></span><br><span class="line">response.xpath().getall()  <span class="comment"># 得到多个元素</span></span><br><span class="line">response.xpath()..extract()[<span class="number">0</span>]  <span class="comment"># 得到一个元素</span></span><br><span class="line">response.xpath()..extract_first() <span class="comment"># 得到一个元素</span></span><br></pre></td></tr></table></figure>

<p>另一种方法是通过<code>Scrapy</code>自带的交互式<code>shell</code>来准确验证<code>XPath</code>语句，具体在命令行操作如下,如果能正确<code>print()</code>输出结果，则表示<code>XPath</code>正确。</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">zmj@ubuntu:~/桌面$ scrapy shell http://www.itcast.cn/channel/teacher.shtml</span><br><span class="line">2020-03-21 07:04:51 [scrapy.utils.log] INFO: Scrapy 2.0.0 started (bot: scrapybot)</span><br><span class="line">......</span><br><span class="line">2020-03-21 07:04:51 [scrapy.core.engine] INFO: Spider opened</span><br><span class="line">2020-03-21 07:04:51 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.itcast.cn/channel/teacher.shtml&gt; (referer: None)</span><br><span class="line">[s] Available Scrapy objects:</span><br><span class="line">[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)</span><br><span class="line">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x7f963e8140d0&gt;</span><br><span class="line">[s]   item       &#123;&#125;</span><br><span class="line">[s]   request    &lt;GET http://www.itcast.cn/channel/teacher.shtml&gt;</span><br><span class="line">[s]   response   &lt;200 http://www.itcast.cn/channel/teacher.shtml&gt;</span><br><span class="line">[s]   settings   &lt;scrapy.settings.Settings object at 0x7f963e80ff10&gt;</span><br><span class="line">[s]   spider     &lt;DefaultSpider <span class="string">&#x27;default&#x27;</span> at 0x7f963e369210&gt;</span><br><span class="line">[s] Useful shortcuts:</span><br><span class="line">[s]   fetch(url[, redirect=True]) Fetch URL and update <span class="built_in">local</span> objects (by default, redirects are followed)</span><br><span class="line">[s]   fetch(req)                  Fetch a scrapy.Request and update <span class="built_in">local</span> objects</span><br><span class="line">[s]   shelp()           Shell <span class="built_in">help</span> (<span class="built_in">print</span> this <span class="built_in">help</span>)</span><br><span class="line">[s]   view(response)    View response <span class="keyword">in</span> a browser</span><br><span class="line">In [1]: res = response.xpath(<span class="string">&quot;//div[@class=&#x27;li_txt&#x27;]/h3/text()&quot;</span>)</span><br><span class="line"></span><br><span class="line">In [2]: <span class="built_in">print</span>(res)</span><br><span class="line">[&lt;Selector xpath=<span class="string">&quot;//div[@class=&#x27;li_txt&#x27;]/h3/text()&quot;</span> data=<span class="string">&#x27;王老师&#x27;</span>&gt;,</span><br><span class="line">&lt;Selector xpath=<span class="string">&quot;//div[@class=&#x27;li_txt&#x27;]/h3/text()&quot;</span> data=<span class="string">&#x27;孙老师&#x27;</span>&gt;,</span><br><span class="line">&lt;Selector xpath=<span class="string">&quot;//div[@class=&#x27;li_txt&#x27;]/h3/text()&quot;</span> data=<span class="string">&#x27;李老师&#x27;</span>&gt;,</span><br><span class="line">...]</span><br></pre></td></tr></table></figure>

<p>最后总结 <code>mySpider/spider/itcast.py</code> 代码如下</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> mySpider.items <span class="keyword">import</span> MyspiderItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ItcastSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;itcast&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;itcast.cn&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.itcast.cn/channel/teacher.shtml&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        items = [] <span class="comment"># 存放老师信息的集合</span></span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> response.xpath(<span class="string">&quot;//div[@class=&#x27;li_txt&#x27;]&quot;</span>):</span><br><span class="line">            item = MyspiderItem()</span><br><span class="line">            <span class="comment">#extract()方法返回的都是unicode字符串</span></span><br><span class="line">            name = each.xpath(<span class="string">&quot;h3/text()&quot;</span>).extract()</span><br><span class="line">            title = each.xpath(<span class="string">&quot;h4/text()&quot;</span>).extract()</span><br><span class="line">            info = each.xpath(<span class="string">&quot;p/text()&quot;</span>).extract()</span><br><span class="line">             <span class="comment">#xpath返回的是包含一个元素的列表</span></span><br><span class="line">            item[<span class="string">&#x27;name&#x27;</span>] = name[<span class="number">0</span>]</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = title[<span class="number">0</span>]</span><br><span class="line">            item[<span class="string">&#x27;info&#x27;</span>] = info[<span class="number">0</span>]</span><br><span class="line">            items.append(item)</span><br><span class="line">        <span class="comment"># 直接返回最后数据</span></span><br><span class="line">        <span class="keyword">return</span> items</span><br></pre></td></tr></table></figure>

<h4 id="步骤四-存储爬取内容"><a href="#步骤四-存储爬取内容" class="headerlink" title="步骤四 存储爬取内容"></a>步骤四 存储爬取内容</h4><p>首先编辑 <code>mySpider/</code> 目录下的 <code>settings.py</code>文件，找到如下代码去除注释。这么做的目的是为了防止某些网站使用了反爬虫策略，进行绕过。</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span> <span class="comment"># 是否遵循robotstxt守则，改为False</span></span><br><span class="line"></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">   <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">&#x27;mySpider.pipelines.MyspiderPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面的 <code>DEFAULT_REQUEST_HEADERS</code> 中的为请求头中的数据，我们可以在浏览器中-&gt;检查元素-&gt;网络-&gt;刷新加载-&gt;查看请求头 中获取数据</p>
<p><img src="https://s1.ax1x.com/2020/03/20/82blOU.png" alt="image"></p>
<p><strong>image</strong></p>
<p><code>Scrapy</code>保存信息的最简单的方法主要有四种，<code>-o</code> 输出指定格式的文件，命令如下：</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl itcast -o teachers.json</span><br></pre></td></tr></table></figure>

<p><code>json lines</code>格式，默认为<code>Unicode</code>编码</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl itcast -o teachers.jsonl</span><br></pre></td></tr></table></figure>

<p><code>xml</code>格式</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl itcast -o teachers.xml</span><br></pre></td></tr></table></figure>

<p><code>csv</code> 逗号表达式，可用<code>Excel</code>打开</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl itcast -o teachers.csv</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/20/82Xcb8.png" alt="csv效果图"></p>
<p><strong>csv 效果图</strong></p>
<p>注意：当保存成<code>json</code>格式时，并不会直接生成中文，还是一长串字符格式，具体效果如下</p>
<p><img src="https://s1.ax1x.com/2020/03/20/82j8iQ.png" alt="json效果图1"></p>
<p><strong>json 效果图 1</strong></p>
<p>为了解决这个问题，我们可以自定义数据保存的格式</p>
<p>编辑管道文件 <code>mySpider/pipelines.py</code>，默认增加的代码如下</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyspiderPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<p>将其修改为如下代码</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyspiderPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.filename = <span class="built_in">open</span>(<span class="string">&quot;teacher.json&quot;</span>,<span class="string">&quot;w&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        text = json.dumps(<span class="built_in">dict</span>(item),ensure_ascii = <span class="literal">False</span>) + <span class="string">&quot;\n&quot;</span></span><br><span class="line">        self.filename.write(<span class="built_in">str</span>(text))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        self.filename.close()</span><br></pre></td></tr></table></figure>

<p>然后回到<code>mySpider/</code>目录下，执行运行命令</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl itcast</span><br></pre></td></tr></table></figure>

<p>此时成功将存储的<code>json</code>数据变为中文格式</p>
<p><img src="https://s1.ax1x.com/2020/03/20/82xd5F.png" alt="json效果图2"></p>
<p><strong>json 效果图 2</strong></p>
<h3 id="Scrapy-实战案例（分页面）"><a href="#Scrapy-实战案例（分页面）" class="headerlink" title="Scrapy 实战案例（分页面）"></a>Scrapy 实战案例（分页面）</h3><hr>
<p>在上一个案例中，介绍了如何爬取单页面的内容，接下来介绍下如何在分页模式进行爬取多页面内容。本次案例目标网站页面如下</p>
<p><img src="https://s1.ax1x.com/2020/03/21/8RAlw9.png" alt="目标"></p>
<p><strong>目标</strong></p>
<h4 id="步骤一-新建爬虫项目-1"><a href="#步骤一-新建爬虫项目-1" class="headerlink" title="步骤一 新建爬虫项目"></a>步骤一 新建爬虫项目</h4><p>新建一个名为 <code>itheimaSpider</code> 的爬虫项目，建议项目名字均以 <code>网站域名+Spider</code>格式命名。</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject itheimaSpider</span><br></pre></td></tr></table></figure>

<h4 id="步骤二-明确抓取目标-1"><a href="#步骤二-明确抓取目标-1" class="headerlink" title="步骤二 明确抓取目标"></a>步骤二 明确抓取目标</h4><ol>
<li>明确目标：爬取<code>http://yun.itheima.com/jishu/index/p/1.html</code>所有分页中的文章标题、介绍、链接、标签、浏览数和日期。</li>
<li>打开编写 <code>itheimaSpider/</code> 目录下的 <code>items.py</code>。</li>
</ol>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ItheimaspiderItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    itheima_title = scrapy.Field()</span><br><span class="line">    itheima_introduce = scrapy.Field()</span><br><span class="line">    itheima_url = scrapy.Field()</span><br><span class="line">    itheima_tag = scrapy.Field()</span><br><span class="line">    itheima_view = scrapy.Field()</span><br><span class="line">    itheima_time = scrapy.Field()</span><br></pre></td></tr></table></figure>

<h4 id="步骤三-制作爬虫代码-1"><a href="#步骤三-制作爬虫代码-1" class="headerlink" title="步骤三 制作爬虫代码"></a>步骤三 制作爬虫代码</h4><p>在 <code>itheimaSpider/</code> 目录下输入命令，自动生成 <code>itheima</code>爬虫文件，注意爬虫文件名不要与项目名一样</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider itheima <span class="string">&quot;itheima.com&quot;</span></span><br></pre></td></tr></table></figure>

<p>编辑 <code>itheimaSpider/spider/itheima.py</code> 文件</p>
<p>从源码分析一下页面跳转链接之间的<code>url</code>的区别</p>
<p><img src="https://s1.ax1x.com/2020/03/21/8RM1TU.png" alt="img"></p>
<p>从这几个链接很容易可以看出，页面之间的跳转链接是由<code>yun.itheima.com</code>和各<code>a</code>标签中的<code>href</code>属性拼接而成的</p>
<p>提取出<code>url</code>相同的部分</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">&quot;http://yun.itheima.com&quot;</span></span><br></pre></td></tr></table></figure>

<p>修改置<code>start_urls</code>值</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start_urls = [<span class="string">&#x27;http://yun.itheima.com/jishu/&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>导入<code>items</code>模块</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> itheimaSpider.items <span class="keyword">import</span> ItheimaspiderItem</span><br></pre></td></tr></table></figure>

<p><code>XPath</code>语法匹配指定内容</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//div[@<span class="keyword">class</span>=<span class="string">&#x27;fl&#x27;</span>]/ul/li/a/h2/text() <span class="comment"># 标题</span></span><br><span class="line">//div[@<span class="keyword">class</span>=<span class="string">&#x27;fl&#x27;</span>]/ul/li/a/p/text() <span class="comment"># 介绍</span></span><br><span class="line">//div[@<span class="keyword">class</span>=<span class="string">&#x27;fl&#x27;</span>]/ul/li/a/@href <span class="comment"># 文章链接</span></span><br><span class="line">//div[@<span class="keyword">class</span>=<span class="string">&#x27;fl&#x27;</span>]/ul/li/a/div/h3/text() <span class="comment"># 标签</span></span><br><span class="line">//div[@<span class="keyword">class</span>=<span class="string">&#x27;fl&#x27;</span>]/ul/li/a/div/p[<span class="number">1</span>]/text() <span class="comment"># 浏览数</span></span><br><span class="line">//div[@<span class="keyword">class</span>=<span class="string">&#x27;fl&#x27;</span>]/ul/li/a/div/p[<span class="number">2</span>]/text() <span class="comment"># 日期</span></span><br><span class="line"></span><br><span class="line">//div[@<span class="keyword">class</span>=<span class="string">&#x27;pagebox&#x27;</span>]/div/a[@<span class="keyword">class</span>=<span class="string">&#x27;next&#x27;</span>]/@href <span class="comment"># 下一页链接</span></span><br></pre></td></tr></table></figure>

<p>提取匹配出每篇文章各自内容的<code>XPath</code>语句部分，作为循环主体</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//div[@<span class="keyword">class</span>=<span class="string">&#x27;fl&#x27;</span>]/ul/li</span><br></pre></td></tr></table></figure>

<p>然后再子循环读取每篇文章下各自的内容</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 循环读取文章主体</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> response.xpath(<span class="string">&quot;//div[@class=&#x27;fl&#x27;]/ul/li&quot;</span>):</span><br><span class="line">    <span class="comment"># 子循环读取文章内容</span></span><br><span class="line">    item = ItheimaspiderItem()</span><br><span class="line">    item[<span class="string">&#x27;itheima_title&#x27;</span>] = line.xpath(<span class="string">&quot;./a/h2/text()&quot;</span>).get() <span class="comment"># 标题</span></span><br><span class="line">    item[<span class="string">&#x27;itheima_introduce&#x27;</span>] = line.xpath(<span class="string">&quot;./a/p/text()&quot;</span>).get() <span class="comment"># 介绍</span></span><br><span class="line">    item[<span class="string">&#x27;itheima_url&#x27;</span>] = self.url + line.xpath(<span class="string">&quot;./a/@href&quot;</span>).get() <span class="comment"># 文章链接</span></span><br><span class="line">    item[<span class="string">&#x27;itheima_tag&#x27;</span>] = line.xpath(<span class="string">&quot;./a/div/h3/text()&quot;</span>).get() <span class="comment"># 标签</span></span><br><span class="line">    item[<span class="string">&#x27;itheima_view&#x27;</span>] = line.xpath(<span class="string">&quot;./a/div/p[1]/text()&quot;</span>).get() <span class="comment"># 浏览数</span></span><br><span class="line">    item[<span class="string">&#x27;itheima_time&#x27;</span>] = line.xpath(<span class="string">&quot;./a/div/p[2]/text()&quot;</span>).get() <span class="comment"># 日期</span></span><br><span class="line">    <span class="comment"># 返回item</span></span><br><span class="line">    <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>

<p>为了能够获取所有分页面的内容，需要自动获取下一页的跳转<code>url</code>，这里使用<code>XPath</code>匹配到下一页<code>a</code>标签的<code>href</code>属性，然后拼接上<code>http://yun.itheima.com</code>即可。</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 翻页操作</span></span><br><span class="line">next_page = response.xpath(<span class="string">&quot;//div[@class=&#x27;pagebox&#x27;]/div/a[@class=&#x27;next&#x27;]/@href&quot;</span>).get()</span><br><span class="line"><span class="keyword">if</span> next_page: <span class="comment"># 判断是否有下一页</span></span><br><span class="line">    <span class="comment"># 拼接下一页的网址</span></span><br><span class="line">    next_url = response.urljoin(next_page)</span><br><span class="line">    <span class="comment"># 发出Request请求，callback回调parse函数</span></span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request(next_url, callback = self.parse)</span><br></pre></td></tr></table></figure>

<p>最后总结下 <code>itheimaSpider/spiders/itheima.py</code> 代码</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itheimaSpider.items <span class="keyword">import</span> ItheimaspiderItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ItheimaSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;itheima&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;itheima.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://yun.itheima.com/jishu/&#x27;</span>]</span><br><span class="line">    url = <span class="string">&quot;http://yun.itheima.com&quot;</span> <span class="comment"># 提取URL相同的部分</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 循环读取文章主体</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> response.xpath(<span class="string">&quot;//div[@class=&#x27;fl&#x27;]/ul/li&quot;</span>):</span><br><span class="line">            <span class="comment"># 子循环读取文章内容</span></span><br><span class="line">            item = ItheimaspiderItem()</span><br><span class="line">            item[<span class="string">&#x27;itheima_title&#x27;</span>] = line.xpath(<span class="string">&quot;./a/h2/text()&quot;</span>).get()</span><br><span class="line">            item[<span class="string">&#x27;itheima_introduce&#x27;</span>] = line.xpath(<span class="string">&quot;./a/p/text()&quot;</span>).get()</span><br><span class="line">            item[<span class="string">&#x27;itheima_url&#x27;</span>] = url + line.xpath(<span class="string">&quot;./a/@href&quot;</span>).get()</span><br><span class="line">            item[<span class="string">&#x27;itheima_tag&#x27;</span>] = line.xpath(<span class="string">&quot;./a/div/h3/text()&quot;</span>).get()</span><br><span class="line">            item[<span class="string">&#x27;itheima_view&#x27;</span>] = line.xpath(<span class="string">&quot;./a/div/p[1]/text()&quot;</span>).get()</span><br><span class="line">            item[<span class="string">&#x27;itheima_time&#x27;</span>] = line.xpath(<span class="string">&quot;./a/div/p[2]/text()&quot;</span>).get()</span><br><span class="line">            <span class="comment"># 返回item</span></span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 翻页操作</span></span><br><span class="line">        next_page = response.xpath(<span class="string">&quot;//div[@class=&#x27;pagebox&#x27;]/div/a[@class=&#x27;next&#x27;]/@href&quot;</span>).get()</span><br><span class="line">        <span class="keyword">if</span> next_page:</span><br><span class="line">            <span class="comment"># 拼接下一页的网址</span></span><br><span class="line">            next_url = response.urljoin(next_page)</span><br><span class="line">            <span class="comment"># 发出Request请求，callback回调parse函数</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(next_url, callback = self.parse)</span><br></pre></td></tr></table></figure>

<h4 id="步骤四-存储爬取内容-1"><a href="#步骤四-存储爬取内容-1" class="headerlink" title="步骤四 存储爬取内容"></a>步骤四 存储爬取内容</h4><p>编辑 <code>itheimaSpider/</code> 目录下的 <code>settings.py</code> 文件，找到如下代码去除注释并修改代码。</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">   <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">&#x27;itheimaSpider.pipelines.ItheimaspiderPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>最后在 <code>itheimaSpider/</code> 输入运行爬虫命令</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl itheima -o itheima.csv</span><br></pre></td></tr></table></figure>

<p>保存数据在 <code>itheima.csv</code> 文件中</p>
<p><img src="https://s1.ax1x.com/2020/03/21/8R1tpQ.png" alt="itheima.csv"></p>
<p><strong>itheima.csv</strong></p>
<h3 id="Scrapy-爬虫进阶（多级页面）"><a href="#Scrapy-爬虫进阶（多级页面）" class="headerlink" title="Scrapy 爬虫进阶（多级页面）"></a>Scrapy 爬虫进阶（多级页面）</h3><hr>
<p>上个案例介绍了如何在多个分页中爬取到我们想要的数据，接下来这个案例将介绍如何在多级页面中获取到我们想要的数据。</p>
<p>本次演示网站为<code>https://www.ivsky.com/bizhi/</code> ，目标是爬取该网页中所有图集中的图片</p>
<p><img src="https://s1.ax1x.com/2020/03/21/8Wo4iR.png" alt="天堂图片网"></p>
<p><strong>天堂图片网</strong></p>
<p>本次为三级页面示例，通过一级壁纸首页，找到二级图集首页，再通过图集找到三级图片页面地址，最终目的是要爬取下载所有图集中的图片。</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">一级：https://www.ivsky.com/bizhi/</span><br><span class="line">    二级：https://www.ivsky.com/bizhi/hudie_v58539/</span><br><span class="line">        三级：https://www.ivsky.com/bizhi/hudie_v58539/pic_921157.html</span><br></pre></td></tr></table></figure>

<h4 id="步骤一-新建爬虫项目-2"><a href="#步骤一-新建爬虫项目-2" class="headerlink" title="步骤一 新建爬虫项目"></a>步骤一 新建爬虫项目</h4><p>新建一个名为 <code>ivskySpider</code> 的爬虫项目</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject ivskySpider</span><br></pre></td></tr></table></figure>

<h4 id="步骤二-明确抓取目标-2"><a href="#步骤二-明确抓取目标-2" class="headerlink" title="步骤二 明确抓取目标"></a>步骤二 明确抓取目标</h4><ol>
<li>明确目标：爬取<code>https://www.ivsky.com/bizhi/</code>所有分页中的图片。</li>
<li>打开编写 <code>ivskySpider/</code> 目录下的 <code>items.py</code>。</li>
</ol>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ItheimaspiderItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># 收集下载图片的url</span></span><br><span class="line">    image_urls = scrapy.Field()</span><br><span class="line">    <span class="comment"># 供给管道下载使用</span></span><br><span class="line">    images = scrapy.Field()</span><br></pre></td></tr></table></figure>

<h4 id="步骤三-制作爬虫代码-2"><a href="#步骤三-制作爬虫代码-2" class="headerlink" title="步骤三 制作爬虫代码"></a>步骤三 制作爬虫代码</h4><p>在 <code>ivskySpider/</code> 目录下输入命令，自动生成 <code>ivsky</code>爬虫文件,<code>-t crawl</code>表示使用<code>crawl</code>的模板。</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider -t crawl ivsky <span class="string">&quot;ivsky.com&quot;</span></span><br></pre></td></tr></table></figure>

<p>打开 <code>ivskySpider/spider/ivsky.py</code> 文件，默认增加的代码如下</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">IvskySpider</span>(<span class="title class_ inherited__">CrawlSpider</span>):</span><br><span class="line">    name = <span class="string">&#x27;ivsky&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;ivsky.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://ivsky.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;Items/&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_item</span>(<span class="params">self, response</span>):</span><br><span class="line">        item = &#123;&#125;</span><br><span class="line">        <span class="comment">#item[&#x27;domain_id&#x27;] = response.xpath(&#x27;//input[@id=&quot;sid&quot;]/@value&#x27;).get()</span></span><br><span class="line">        <span class="comment">#item[&#x27;name&#x27;] = response.xpath(&#x27;//div[@id=&quot;name&quot;]&#x27;).get()</span></span><br><span class="line">        <span class="comment">#item[&#x27;description&#x27;] = response.xpath(&#x27;//div[@id=&quot;description&quot;]&#x27;).get()</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<p>导入 <code>items.py</code> 文件</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ivskySpider.items <span class="keyword">import</span> IvskyspiderItem</span><br></pre></td></tr></table></figure>

<p>修改 <code>start_urls</code> 值如下</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start_urls = [<span class="string">&#x27;https://www.ivsky.com/bizhi/&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>先分析一下总共需要获取的<code>url</code>：首页翻页<code>url</code>，图集翻页<code>url</code>，以及图片链接<code>url</code>的格式，进行正则匹配获取，然后编写<code>rules</code>规则匹配</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">分析获取url逻辑顺序：</span><br><span class="line">首页 url：www.ivsky.com/bizhi/</span><br><span class="line">  首页翻页：不回调 url：https://www.ivsky.com/bizhi/index_d+.html</span><br><span class="line">    图集首页：不回调 url：https://www.ivsky.com/bizhi/\w+_v\d+/</span><br><span class="line">      图集翻页-》需回调：提取图片url，供下载使用 url：https://www.ivsky.com/bizhi/\w+_v\d+/pic_d+.html</span><br><span class="line"></span><br><span class="line">写rule规则时与逻辑顺序相反：</span><br><span class="line">rule图集翻页-》需回调：提取图片url，供下载使用 url：https://www.ivsky.com/bizhi/\w+_v\d+/pic_d+.html</span><br><span class="line">  rule图集首页：不回调 url：https://www.ivsky.com/bizhi/\w+_v\d+/</span><br><span class="line">    rule首页翻页：不回调 url：https://www.ivsky.com/bizhi/index_d+.html</span><br><span class="line">      首页 url：www.ivsky.com/bizhi/</span><br></pre></td></tr></table></figure>

<p>修改 <code>rules</code> 规则如下，其中 <code>allow</code> 值中的 <code>\d+</code> 代表匹配 1 次或多任意数字值， <code>\w+</code> 代表匹配 1 次或多次任意字符值，然后会自动匹配当前页面中所有符合该规则的<code>url</code>格式，更多规则点击查看<a target="_blank" rel="noopener" href="https://deerchao.cn/tutorials/regex/regex.htm">正则匹配</a></p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rules = (</span><br><span class="line">    Rule(LinkExtractor(allow=<span class="string">r&#x27;https://www.ivsky.com/bizhi/\w+_v\d+/pic_d+.html&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">    Rule(LinkExtractor(allow=<span class="string">r&#x27;https://www.ivsky.com/bizhi/\w+_v\d+/&#x27;</span>), follow=<span class="literal">True</span>),</span><br><span class="line">    Rule(LinkExtractor(allow=<span class="string">r&#x27;https://www.ivsky.com/bizhi/index_d+.html&#x27;</span>), follow=<span class="literal">True</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>然后<code>XPath</code>匹配图片下载的链接</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//div[@<span class="built_in">id</span>=<span class="string">&#x27;pic_con&#x27;</span>]/div/img/@src</span><br><span class="line"></span><br><span class="line">response.xpath(<span class="string">&quot;//div[@id=&#x27;pic_con&#x27;]/div/img/@src&quot;</span>).get()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意：笔者这里<code>XPath</code>遇到个坑，之前因为过分相信<code>XPath Helper</code>的验证机制，导致在这踩到坑了。有时候<code>XPath Helper</code>在谷歌浏览器中能正确匹配出想要的内容，但是到了<code>Scrapy</code>中却无法匹配到正确结果。这里强调如果遇到这种情况，建议手动在<code>Scrapy shell</code>中重新使用的<code>XPath</code>语法进行匹配，最后以此为正确结果。具体的情况请看本文结尾的<code>小贴士</code>分析。</p>
</blockquote>
<p>修改 <code>parse_item()</code> 方法如下，主要是将获取的图片<code>url</code>进行<code>list</code>数组返回</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse_item</span>(<span class="params">self, response</span>):</span><br><span class="line">        item = IvskyspiderItem()</span><br><span class="line">        image_url = <span class="string">&quot;http:&quot;</span> + response.xpath(<span class="string">&quot;//div[@id=&#x27;pic_con&#x27;]/div/img/@src&quot;</span>).get()</span><br><span class="line">        image_list = []</span><br><span class="line">        image_list.append(image_url)</span><br><span class="line">        item[<span class="string">&#x27;image_urls&#x27;</span>] = image_list</span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>

<p>最后总结 <code>ivskySpider/spider/ivsky.py</code> 代码如下</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> ivskySpider.items <span class="keyword">import</span> IvskyspiderItem</span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">IvskySpider</span>(<span class="title class_ inherited__">CrawlSpider</span>):</span><br><span class="line">    name = <span class="string">&#x27;ivsky&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;ivsky.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.ivsky.com/bizhi/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;https://www.ivsky.com/bizhi/\w+_v\d+/pic_\d+.html&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;https://www.ivsky.com/bizhi/\w+_v\d+/&#x27;</span>), follow=<span class="literal">True</span>),</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;https://www.ivsky.com/bizhi/index_\d+.html&#x27;</span>), follow=<span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_item</span>(<span class="params">self, response</span>):</span><br><span class="line">        item = IvskyspiderItem()</span><br><span class="line">        image_url = <span class="string">&quot;http:&quot;</span> + response.xpath(<span class="string">&quot;//div[@id=&#x27;pic_con&#x27;]/div/img/@src&quot;</span>).get()</span><br><span class="line">        image_list = []</span><br><span class="line">        image_list.append(image_url)</span><br><span class="line">        item[<span class="string">&#x27;image_urls&#x27;</span>] = image_list</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>

<h4 id="步骤四-存储爬取内容-2"><a href="#步骤四-存储爬取内容-2" class="headerlink" title="步骤四 存储爬取内容"></a>步骤四 存储爬取内容</h4><p>编辑 <code>ivskySpider/</code> 目录下的 <code>settings.py</code> 文件，找到如下代码去除注释并修改代码。</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">   <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;Referer&#x27;</span>: <span class="string">&#x27;https://www.ivsky.com&#x27;</span>, <span class="comment"># 防止网站防跨域</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">&#x27;scrapy.pipelines.images.ImagesPipeline&#x27;</span>: <span class="number">300</span>, <span class="comment"># 开启图片下载管道</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加代码设置下载路径</span></span><br><span class="line">IMAGES_STORE = <span class="string">&#x27;img&#x27;</span></span><br></pre></td></tr></table></figure>

<p>最后在 <code>ivskySpider/</code> 输入运行爬虫命令</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl ivsky</span><br></pre></td></tr></table></figure>

<p>保存数据在 <code>ivskySpider/img/full</code> 目录下</p>
<p><img src="https://s1.ax1x.com/2020/03/21/8huAdf.png" alt="ivskyImages"></p>
<p><strong>ivskyImages</strong></p>
<h3 id="小贴士"><a href="#小贴士" class="headerlink" title="小贴士"></a>小贴士</h3><hr>
<p>这里着重提示真正的<code>XPath</code>语句以<code>Scrapy shell</code>中的语句为准，在其他的<code>XPath</code>插件中的语句仅供参考。</p>
<p>例如下面这种情况：两个<code>XPath</code>语句目的是为了匹配出图片的链接地址</p>
<p>首先是这句<code>XPath</code>，使用谷歌的<code>XPath Helper</code>插件匹配是正确的，没有问题</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//a[@<span class="keyword">class</span>=<span class="string">&#x27;page-next&#x27;</span>]/img/@src</span><br></pre></td></tr></table></figure>

<p><img src="https://s1.ax1x.com/2020/03/21/8f1BAP.png" alt="XPath Helper"></p>
<p><strong>XPath Helper</strong></p>
<p>但是同样的这句话，到了<code>Scrapy shell</code>中却失效了，没有匹配到任何信息</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">zmj@ubuntu:~/桌面$ scrapy shell https://www.ivsky.com/bizhi/hudie_v58539/pic_921157.html</span><br><span class="line">......</span><br><span class="line">2020-03-21 05:10:15 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://www.ivsky.com/bizhi/hudie_v58539/pic_921157.html&gt; (referer: None)</span><br><span class="line">[s] Available Scrapy objects:</span><br><span class="line">[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)</span><br><span class="line">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x7fead038d610&gt;</span><br><span class="line">[s]   item       &#123;&#125;</span><br><span class="line">[s]   request    &lt;GET https://www.ivsky.com/bizhi/hudie_v58539/pic_921157.html&gt;</span><br><span class="line">[s]   response   &lt;200 https://www.ivsky.com/bizhi/hudie_v58539/pic_921157.html&gt;</span><br><span class="line">[s]   settings   &lt;scrapy.settings.Settings object at 0x7fead038d210&gt;</span><br><span class="line">[s]   spider     &lt;DefaultSpider <span class="string">&#x27;default&#x27;</span> at 0x7feacfee26d0&gt;</span><br><span class="line">[s] Useful shortcuts:</span><br><span class="line">[s]   fetch(url[, redirect=True]) Fetch URL and update <span class="built_in">local</span> objects (by default, redirects are followed)</span><br><span class="line">[s]   fetch(req)                  Fetch a scrapy.Request and update <span class="built_in">local</span> objects</span><br><span class="line">[s]   shelp()           Shell <span class="built_in">help</span> (<span class="built_in">print</span> this <span class="built_in">help</span>)</span><br><span class="line">[s]   view(response)    View response <span class="keyword">in</span> a browser</span><br><span class="line">In [1]: url = response.xpath(<span class="string">&quot;//a[@class=&#x27;page-next&#x27;]/img/@src&quot;</span>).get()</span><br><span class="line"></span><br><span class="line">In [2]: <span class="built_in">print</span>(url)</span><br><span class="line">None</span><br><span class="line"></span><br><span class="line">In [3]:</span><br></pre></td></tr></table></figure>

<p>既然<code>scrapy</code>不认这句<code>XPath</code>，那么只好在<code>scrapy shell</code>重新手动匹配</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [3]: url = response.xpath(<span class="string">&quot;//div[@id=&#x27;pic_con&#x27;]&quot;</span>).get()</span><br><span class="line"></span><br><span class="line">In [4]: <span class="built_in">print</span>(url)</span><br><span class="line">&lt;div <span class="built_in">id</span>=<span class="string">&quot;pic_con&quot;</span>&gt;&lt;div&gt;&lt;script&gt;dy(<span class="string">&quot;pic_tonext&quot;</span>);&lt;/script&gt;&lt;img <span class="built_in">id</span>=<span class="string">&quot;imgis&quot;</span> src=<span class="string">&quot;//img.ivsky.com/img/bizhi/pre/201910/07/hudie.jpg&quot;</span> alt=<span class="string">&quot;美丽可爱的蝴蝶图片&quot;</span>&gt;&lt;/div&gt;&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">In [5]: url = response.xpath(<span class="string">&quot;//div[@id=&#x27;pic_con&#x27;]/div/img/@src&quot;</span>).get()</span><br><span class="line"></span><br><span class="line">In [6]: <span class="built_in">print</span>(url)</span><br><span class="line">//img.ivsky.com/img/bizhi/pre/201910/07/hudie.jpg</span><br><span class="line"></span><br><span class="line">In [7]:</span><br></pre></td></tr></table></figure>

<p>最后匹配到的真正<code>url</code>的<code>XPath</code>语句为</p>
<p>bash</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//div[@<span class="built_in">id</span>=<span class="string">&#x27;pic_con&#x27;</span>]/div/img/@src</span><br><span class="line"></span><br><span class="line">response.xpath(<span class="string">&quot;//div[@id=&#x27;pic_con&#x27;]/div/img/@src&quot;</span>).get()</span><br></pre></td></tr></table></figure>

<p>同样的再将<code>scrapy shell</code>匹配出的这句<code>XPath</code>放到谷歌浏览器中也没法匹配到<code>url</code></p>
<p>造成这个原因是使用<code>Scrapy</code>在爬取目标网站时与在浏览器访问目标网站时的源码有些差距。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Dream星辰</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/10/14/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/">http://example.com/2023/10/14/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">小邹同学</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://img1.baidu.com/it/u=2628701950,2035046880&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=500&amp;h=288" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/10/14/vue3/"><img class="prev-cover" src="https://img2.baidu.com/it/u=4045139888,320238652&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=889&amp;h=500" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">vue3</div></div></a></div><div class="next-post pull-right"><a href="/2023/10/14/vue3%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"><img class="next-cover" src="https://img2.baidu.com/it/u=4045139888,320238652&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=889&amp;h=500" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">vue3基础学习</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/logo1.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Dream星辰</div><div class="author-info__description">本网站是由本人在githun上需寻找的开源代码，经过漫长的改造最归成为我的个人博客</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Dream935"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Dream935" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/705037868@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Scrapy-%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">Scrapy 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scrapy-%E6%9E%B6%E6%9E%84"><span class="toc-number">2.</span> <span class="toc-text">Scrapy 架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scrapy-%E5%8E%9F%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">Scrapy 原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scrapy-%E5%AE%89%E8%A3%85"><span class="toc-number">4.</span> <span class="toc-text">Scrapy 安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scrapy-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%EF%BC%88%E5%8D%95%E9%A1%B5%E9%9D%A2%EF%BC%89"><span class="toc-number">5.</span> <span class="toc-text">Scrapy 实战案例（单页面）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E4%B8%80-%E6%96%B0%E5%BB%BA%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE"><span class="toc-number">5.1.</span> <span class="toc-text">步骤一 新建爬虫项目</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E4%BA%8C-%E6%98%8E%E7%A1%AE%E6%8A%93%E5%8F%96%E7%9B%AE%E6%A0%87"><span class="toc-number">5.2.</span> <span class="toc-text">步骤二 明确抓取目标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E4%B8%89-%E5%88%B6%E4%BD%9C%E7%88%AC%E8%99%AB%E4%BB%A3%E7%A0%81"><span class="toc-number">5.3.</span> <span class="toc-text">步骤三 制作爬虫代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E5%9B%9B-%E5%AD%98%E5%82%A8%E7%88%AC%E5%8F%96%E5%86%85%E5%AE%B9"><span class="toc-number">5.4.</span> <span class="toc-text">步骤四 存储爬取内容</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scrapy-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%EF%BC%88%E5%88%86%E9%A1%B5%E9%9D%A2%EF%BC%89"><span class="toc-number">6.</span> <span class="toc-text">Scrapy 实战案例（分页面）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E4%B8%80-%E6%96%B0%E5%BB%BA%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE-1"><span class="toc-number">6.1.</span> <span class="toc-text">步骤一 新建爬虫项目</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E4%BA%8C-%E6%98%8E%E7%A1%AE%E6%8A%93%E5%8F%96%E7%9B%AE%E6%A0%87-1"><span class="toc-number">6.2.</span> <span class="toc-text">步骤二 明确抓取目标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E4%B8%89-%E5%88%B6%E4%BD%9C%E7%88%AC%E8%99%AB%E4%BB%A3%E7%A0%81-1"><span class="toc-number">6.3.</span> <span class="toc-text">步骤三 制作爬虫代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E5%9B%9B-%E5%AD%98%E5%82%A8%E7%88%AC%E5%8F%96%E5%86%85%E5%AE%B9-1"><span class="toc-number">6.4.</span> <span class="toc-text">步骤四 存储爬取内容</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scrapy-%E7%88%AC%E8%99%AB%E8%BF%9B%E9%98%B6%EF%BC%88%E5%A4%9A%E7%BA%A7%E9%A1%B5%E9%9D%A2%EF%BC%89"><span class="toc-number">7.</span> <span class="toc-text">Scrapy 爬虫进阶（多级页面）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E4%B8%80-%E6%96%B0%E5%BB%BA%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE-2"><span class="toc-number">7.1.</span> <span class="toc-text">步骤一 新建爬虫项目</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E4%BA%8C-%E6%98%8E%E7%A1%AE%E6%8A%93%E5%8F%96%E7%9B%AE%E6%A0%87-2"><span class="toc-number">7.2.</span> <span class="toc-text">步骤二 明确抓取目标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E4%B8%89-%E5%88%B6%E4%BD%9C%E7%88%AC%E8%99%AB%E4%BB%A3%E7%A0%81-2"><span class="toc-number">7.3.</span> <span class="toc-text">步骤三 制作爬虫代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E5%9B%9B-%E5%AD%98%E5%82%A8%E7%88%AC%E5%8F%96%E5%86%85%E5%AE%B9-2"><span class="toc-number">7.4.</span> <span class="toc-text">步骤四 存储爬取内容</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E8%B4%B4%E5%A3%AB"><span class="toc-number">8.</span> <span class="toc-text">小贴士</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/10/14/vue3/" title="vue3"><img src="https://img2.baidu.com/it/u=4045139888,320238652&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=889&amp;h=500" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="vue3"/></a><div class="content"><a class="title" href="/2023/10/14/vue3/" title="vue3">vue3</a><time datetime="2023-10-14T15:02:11.810Z" title="发表于 2023-10-14 23:02:11">2023-10-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/14/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/" title="爬虫学习"><img src="https://img1.baidu.com/it/u=2628701950,2035046880&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=500&amp;h=288" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="爬虫学习"/></a><div class="content"><a class="title" href="/2023/10/14/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/" title="爬虫学习">爬虫学习</a><time datetime="2023-10-14T15:01:34.323Z" title="发表于 2023-10-14 23:01:34">2023-10-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/14/vue3%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/" title="vue3基础学习"><img src="https://img2.baidu.com/it/u=4045139888,320238652&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=889&amp;h=500" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="vue3基础学习"/></a><div class="content"><a class="title" href="/2023/10/14/vue3%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/" title="vue3基础学习">vue3基础学习</a><time datetime="2023-10-14T15:01:34.320Z" title="发表于 2023-10-14 23:01:34">2023-10-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/14/python/" title="python基础"><img src="https://pic.rmb.bdstatic.com/bjh/down/97bad9500b845ce3144b3702fb951983.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="python基础"/></a><div class="content"><a class="title" href="/2023/10/14/python/" title="python基础">python基础</a><time datetime="2023-10-14T15:01:34.318Z" title="发表于 2023-10-14 23:01:34">2023-10-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/14/markdown%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE/" title="markdown文件配置"><img src="https://img0.baidu.com/it/u=3425868493,3104015061&amp;fm=253&amp;fmt=auto&amp;app=120&amp;f=JPEG?w=1199&amp;h=800" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="markdown文件配置"/></a><div class="content"><a class="title" href="/2023/10/14/markdown%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE/" title="markdown文件配置">markdown文件配置</a><time datetime="2023-10-14T15:01:34.314Z" title="发表于 2023-10-14 23:01:34">2023-10-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 By Dream星辰</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">I wish you to become your own sun, no need to rely on who's light.<p><a target="_blank" href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为Hexo"></a>&nbsp;<a target="_blank" href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用butterfly"></a>&nbsp;<a target="_blank" href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr" title="本站使用JsDelivr为静态资源提供CDN加速"></a> &nbsp;<a target="_blank" href="https://vercel.com/ "><img src="https://img.shields.io/badge/Hosted-Vervel-brightgreen?style=flat&logo=Vercel" title="本站采用双线部署，默认线路托管于Vercel"></a>&nbsp;<a target="_blank" href="https://vercel.com/ "><img src="https://img.shields.io/badge/Hosted-Coding-0cedbe?style=flat&logo=Codio" title="本站采用双线部署，联通线路托管于Coding"></a>&nbsp;<a target="_blank" href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由Gtihub托管"></a>&nbsp;<a target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="true" data-text="小邹,同学,你,真棒" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>